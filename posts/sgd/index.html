<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions - Wassim Seifeddine</title><meta name=Description content="Talking about Machine learning, mostly computational biology, bio foundation models and large scale training"><meta property="og:url" content="https://wassimseifeddine.com/posts/sgd/">
<meta property="og:site_name" content="Wassim Seifeddine"><meta property="og:title" content="[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions"><meta property="og:description" content="You probably heard the hype around Gradient Descent & how it’s used to optimize Deep Learning Models. But you may never knew that it can be used to minimize ANY function you know.
In this blog post we’re going to see how to use Gradient Descent to optimize a simple quadratic function we all know from highschool.
$ax^2 + bx + c$
with a = 1 b = -40 c = 400 we get the following formula"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2016-10-02T17:54:04+01:00"><meta property="article:modified_time" content="2016-10-02T17:54:04+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions"><meta name=twitter:description content="You probably heard the hype around Gradient Descent & how it’s used to optimize Deep Learning Models. But you may never knew that it can be used to minimize ANY function you know.
In this blog post we’re going to see how to use Gradient Descent to optimize a simple quadratic function we all know from highschool.
$ax^2 + bx + c$
with a = 1 b = -40 c = 400 we get the following formula"><meta name=twitter:site content="@https://twitter.com/WassSeif"><meta name=application-name content="Wassim Seifeddine"><meta name=apple-mobile-web-app-title content="Wassim Seifeddine"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://wassimseifeddine.com/posts/sgd/><link rel=next href=https://wassimseifeddine.com/posts/physics_tf/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/wassimseifeddine.com\/posts\/sgd\/"},"genre":"posts","wordcount":433,"url":"https:\/\/wassimseifeddine.com\/posts\/sgd\/","datePublished":"2016-10-02T17:54:04+01:00","dateModified":"2016-10-02T17:54:04+01:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Author"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"light"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"light"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Wassim Seifeddine">Wassim Seifeddine</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/><i class="fa-solid fa-address-card"></i> About </a><a class=menu-item href=/portfolio/><i class="fa-solid fa-pepper-hot"></i> Portfolio </a><a class=menu-item href=/posts/><i class="fa-solid fa-candy-cane"></i> Posts </a><a class=menu-item href=/conferences/><i class="fa-solid fa-handshake"></i> Conferences </a><a class=menu-item href="https://scholar.google.com/citations?user=x6EK09kAAAAJ&amp;hl=en" rel="noopener noreffer" target=_blank><i class="fa-solid fa-magnifying-glass"></i> Publications </a><a class=menu-item href=/seifeddine_wassim_cv.pdf><i class="fa-solid fa-candy-cane"></i> Resume </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Wassim Seifeddine">Wassim Seifeddine</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title><i class="fa-solid fa-address-card"></i>About</a><a class=menu-item href=/portfolio/ title><i class="fa-solid fa-pepper-hot"></i>Portfolio</a><a class=menu-item href=/posts/ title><i class="fa-solid fa-candy-cane"></i>Posts</a><a class=menu-item href=/conferences/ title><i class="fa-solid fa-handshake"></i>Conferences</a><a class=menu-item href="https://scholar.google.com/citations?user=x6EK09kAAAAJ&amp;hl=en" title rel="noopener noreffer" target=_blank><i class="fa-solid fa-magnifying-glass"></i>Publications</a><a class=menu-item href=/seifeddine_wassim_cv.pdf title><i class="fa-solid fa-candy-cane"></i>Resume</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Author</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2016-10-02>2016-10-02</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;433 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;3 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><ul><li><a href=#gradient-descent>Gradient Descent</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>You probably heard the hype around Gradient Descent & how it&rsquo;s used to optimize <em>Deep Learning Models</em>. But you may never knew that it can be used to minimize <strong>ANY</strong> function you know.</p><p>In this blog post we&rsquo;re going to see how to use Gradient Descent to optimize a simple quadratic function</p><p>we all know from highschool.</p><div style=text-align:center><p>$ax^2 + bx + c$</p></div>with<div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>a</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl><span class=n>b</span> <span class=o>=</span> <span class=o>-</span><span class=mi>40</span>
</span></span><span class=line><span class=cl><span class=n>c</span> <span class=o>=</span> <span class=mi>400</span>
</span></span></code></pre></div><p>we get the following formula<br></p><div style=text-align:center>$x^2 - 40x + 400$</div>which can be written as<div style=text-align:center>$(x - 20)^2$</div><br><p>Plotting this function will give us this curve:
<img class=lazyload src=/svg/loading.min.svg data-src=./quad2.png data-srcset="./quad2.png, ./quad2.png 1.5x, ./quad2.png 2x" data-sizes=auto alt=./quad2.png title=quadratic>.</p><p>From the above figure, we can see that this function achieves minimum at <code>x = 20</code>. since $(20 - 20)^2 = 0$</p><p>But suppose this function is a high dimensional function that we can&rsquo;t simply figure out the minimum, these kind of problems requires some algorithm to find (at least try) to achieve this minimum. Here comes gradient descent!</p><h3 id=gradient-descent>Gradient Descent</h3><p>What Gradient Descent does is the following:</p><ol><li>Start at a random value.</li><li>Calculates the derivative ( slope ) of the function at this point.</li><li>Descent in the opposite direction.</li><li>Repeat 1->3 until convergence ( slope ~ 0 )</li></ol><p>Suppose the formula
$(x - 20)^2$</p><p>is our cost function in a machine learning problem that we&rsquo;re trying to minimize.</p><h4 id=implementation>Implementation</h4><p>We start by importing the needed libraries, which are <code>numpy</code> & <code>tensorflow</code></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>tensorflow</span> <span class=k>as</span> <span class=nn>tf</span>
</span></span></code></pre></div><p>then we start tensorflow Interactive Session.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>sess</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>InteractiveSession</span><span class=p>()</span>
</span></span></code></pre></div><p>We are trying to find a value for the parameters X that will minimize the cost function, so let&rsquo;s declare this parameter and initialize it to 0</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>X</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>Variable</span><span class=p>([</span><span class=mi>0</span><span class=p>],</span><span class=n>dtype</span><span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>float32</span><span class=p>)</span>
</span></span></code></pre></div><p>The function we are trying to minimize is Y written as $Y = X^2 - 40X + 400$
So let&rsquo;s write it too</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>Y</span> <span class=o>=</span> <span class=p>(</span><span class=n>X</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span> <span class=o>-</span> <span class=mi>40</span><span class=o>*</span><span class=n>X</span> <span class=o>+</span> <span class=mi>400</span>
</span></span></code></pre></div><p>We are going to use Tensorflow&rsquo;s implementation of Gradient Descent with a <em>learning rate</em> of 0.01</p><blockquote><p><strong>Learning Rate</strong>: We are not going to discuss it here, but think about it as the magnitude of our optimization. How <em><strong>steep</strong></em> is the optimizer going to descent down to the minimum at each iteration.</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>train</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>train</span><span class=o>.</span><span class=n>GradientDescentOptimizer</span><span class=p>(</span><span class=n>learning_rate</span><span class=o>=</span><span class=mf>0.01</span><span class=p>)</span><span class=o>.</span><span class=n>minimize</span><span class=p>(</span><span class=n>Y</span><span class=p>)</span>
</span></span></code></pre></div><p>In Tensorflow, variables are not initialized unless you explicitly do that</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>init</span> <span class=o>=</span> <span class=n>tf</span><span class=o>.</span><span class=n>global_variables_initializer</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>init</span><span class=p>)</span>
</span></span></code></pre></div><p>Now for the training part, we&rsquo;re going to run the optimizer 1000 times to try to get to our minimum</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>number_of_iterations</span> <span class=o>=</span> <span class=mi>1000</span>    
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>number_of_iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>sess</span><span class=o>.</span><span class=n>run</span><span class=p>(</span><span class=n>train</span><span class=p>)</span>
</span></span></code></pre></div><p>Finally after our training is complete, let&rsquo;s see what the optimizer found a value for X to minimize Y</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>X</span><span class=o>.</span><span class=n>eval</span><span class=p>())</span>
</span></span></code></pre></div><p>we get <code>19.99995422</code> which is approximately 20!</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2016-10-02</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://wassimseifeddine.com/posts/sgd/ data-title="[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions" data-via=https://twitter.com/WassSeif><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://wassimseifeddine.com/posts/sgd/><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://wassimseifeddine.com/posts/sgd/ data-title="[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://wassimseifeddine.com/posts/sgd/ data-title="[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://wassimseifeddine.com/posts/sgd/ data-title="[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/physics_tf/ class=next rel=next title="[ARCHIVE] Learning High School Physics with Tensorflow">[ARCHIVE] Learning High School Physics with Tensorflow<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JKG8HGHF8S",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-JKG8HGHF8S" async></script></body></html>