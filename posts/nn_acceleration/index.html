<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Neural network acceleration - Wassim Seifeddine</title><meta name=Description content="Exploration of neural network acceleration techniques including quantization, pruning, and other methods to reduce model size and improve performance."><meta property="og:url" content="https://wassimseifeddine.com/posts/nn_acceleration/"><meta property="og:site_name" content="Wassim Seifeddine"><meta property="og:title" content="Neural network acceleration"><meta property="og:description" content="Exploration of neural network acceleration techniques including quantization, pruning, and other methods to reduce model size and improve performance."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-12-25T17:55:13+02:00"><meta property="article:modified_time" content="2022-12-25T17:55:13+02:00"><meta property="article:tag" content="Quantization"><meta property="article:tag" content="Pruning"><meta property="og:image" content="https://wassimseifeddine.com/profile.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://wassimseifeddine.com/profile.jpeg"><meta name=twitter:title content="Neural network acceleration"><meta name=twitter:description content="Exploration of neural network acceleration techniques including quantization, pruning, and other methods to reduce model size and improve performance."><meta name=application-name content="Wassim Seifeddine"><meta name=apple-mobile-web-app-title content="Wassim Seifeddine"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://wassimseifeddine.com/posts/nn_acceleration/><link rel=prev href=https://wassimseifeddine.com/posts/torchserve/><link rel=next href=https://wassimseifeddine.com/posts/intro/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Neural network acceleration","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/wassimseifeddine.com\/posts\/nn_acceleration\/"},"image":["https:\/\/wassimseifeddine.com\/profile.jpeg"],"genre":"posts","keywords":"Quantization, Pruning","wordcount":816,"url":"https:\/\/wassimseifeddine.com\/posts\/nn_acceleration\/","datePublished":"2022-12-25T17:55:13+02:00","dateModified":"2022-12-25T17:55:13+02:00","publisher":{"@type":"Organization","name":"Wassim Seifeddine","logo":"https:\/\/wassimseifeddine.com\/profile.jpeg"},"author":{"@type":"Person","name":"Author"},"description":"Exploration of neural network acceleration techniques including quantization, pruning, and other methods to reduce model size and improve performance."}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"light"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"light"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Wassim Seifeddine">Wassim Seifeddine</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/>About </a><a class=menu-item href=/posts/>Posts </a><a class=menu-item href="https://scholar.google.com/citations?user=x6EK09kAAAAJ&amp;hl=en" rel="noopener noreffer" target=_blank>Publications </a><a class=menu-item href=/seifeddine_wassim_cv.pdf>Resume </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i>
</span></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Wassim Seifeddine">Wassim Seifeddine</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i>
</a><a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i>
</a><span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/about/ title>About</a><a class=menu-item href=/posts/ title>Posts</a><a class=menu-item href="https://scholar.google.com/citations?user=x6EK09kAAAAJ&amp;hl=en" title rel="noopener noreffer" target=_blank>Publications</a><a class=menu-item href=/seifeddine_wassim_cv.pdf title>Resume</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Neural network acceleration</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Author</a></span>&nbsp;<span class=post-category>included in <a href=/categories/deep-learning/><i class="far fa-folder fa-fw" aria-hidden=true></i>Deep Learning</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-12-25>2022-12-25</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;816 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#why-accelerate>Why accelerate</a></li><li><a href=#accelerating-training-vs-inference>Accelerating Training vs Inference</a><ul><li><a href=#training>Training</a></li><li><a href=#inference>Inference</a></li></ul></li><li><a href=#techniques>Techniques</a></li><li><a href=#quantization>Quantization</a><ul><li><a href=#description>Description:</a></li><li><a href=#how-to-do-it>How to do it:</a></li></ul></li><li><a href=#references>References</a></li></ul></nav></div></div><div class=content id=content><h2 id=why-accelerate>Why accelerate</h2><p>Well, we have neural networks, they are awesome, they work but there&rsquo;s a problem. THEY ARE <strong>HUGE</strong>. We scaled from a hundred of millions of parameters to hundred of <strong>BILLIONS</strong>. This problem makes using neural networks in real life quite hard as you normally don&rsquo;t have this huge computational capabilities to run them anywhere.</p><p>Neural networks have proven to be a very valuable tool in scenarios where the transformation from inputs to outputs is unknown. Suppose you are asked to write an algorithm to classify an image if it&rsquo;s a cat or a dog, how would you do that ? Well first you might ask yourself, &ldquo;what makes an image a cat?&rdquo;. Answering this question is incredibly hard because a vast amount of cases to cover in order to have your algorithm generalizable. This is where neural networks shine; Given an input $ x_{i} $ with its respective label $ y_{i}$ you can use a neural network model with a set of parameters $\theta$ denoted by $ M(\theta) $ to approximate $y_{i} = f(x_{i})$. Normally with enough data you can get a very good estimate of $f$. <strong>However</strong>, this comes at a huge cost, training and running these large networks is expensive in terms of time and memory because of the huge amount of parameters that you need to learn to get the best approximation, this makes these models hard to use in real life scenarios. Also, the recent trend of models getting bigger and bigger in order to get better performance is making this problem even harder.</p><p><figure><a class=lightgallery href=./01_model_size.jpeg title="Mode Size" data-thumbnail=./01_model_size.jpeg data-sub-html="<h2>Model size for language models</h2><p>Mode Size</p>"><img class=lazyload src=/svg/loading.min.svg data-src=./01_model_size.jpeg data-srcset="./01_model_size.jpeg, ./01_model_size.jpeg 1.5x, ./01_model_size.jpeg 2x" data-sizes=auto alt=./01_model_size.jpeg></a><figcaption class=image-caption>Model size for language models</figcaption></figure></p><p>There has been a lot of effort in trying to accelerate these models. Let&rsquo;s talk more about this.</p><h2 id=accelerating-training-vs-inference>Accelerating Training vs Inference</h2><p>It&rsquo;s important to note something. Accelerating neural networks comes in two forms. Accelerating <strong>training</strong> and/or accelerating <strong>inference</strong>.</p><h3 id=training>Training</h3><p>Accelerating training is the process of speeding up the <strong>learning</strong> of the parameters of the neural network. This is normally more tricky to get right than accelerating inference.</p><h3 id=inference>Inference</h3><p>Accelerating inference is the process of speeding up the <strong>running</strong> of a neural network. Normally the process is that you take a <strong>trained</strong> <em>slow</em> model, and then you try to make it run faster while preserving the same performance.</p><p>Both techniques are very helpful because they bring us closer to using these model in real life applications, however, it&rsquo;s more interesting to look at the
acceleration of the training of the neural network or the combination of both.</p><h2 id=techniques>Techniques</h2><p>There are a lot of techniques to accelerate neural networks. I can only discuss quantization as this is the one I&rsquo;m most familiar with. I will try to cover the other techniques in future edits.</p><p>Some of the techniques are:</p><ol><li>Layer/Weight Pruning.</li><li><strong>Quantization</strong>.</li><li>Knowledge Distillation.</li><li>Parameter Sharing.</li><li>Tensor decomposition.</li></ol><h2 id=quantization>Quantization</h2><h3 id=description>Description:</h3><p>Quantization<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> is an very interesting field of model compression it&rsquo;s personally my favorite and the one I&rsquo;m most experienced in so I&rsquo;ll discuss it the most. To undertand quantization we need to understand the concept of numbers on a computer.</p><p><strong>What are numbers</strong>:</p><p>For us number are just a set of symbols that we use to represent a value. We see and treat numbers in the decimal system (base 10) but there are other systems like binary (base 2) or hexadecimal (base 16). For computers numbers are represented in binary (base 2) which has only 1&rsquo;s and 0&rsquo;s. There has to be some standardization on how numbers are represented because it might become very messy and we can&rsquo;t share computations across systems. This is where the <a href=https://en.wikipedia.org/wiki/IEEE_754 target=_blank rel="noopener noreffer">IEEE 754 standard</a> comes in. This standard defines how numbers are represented in binary the most common representation is the <strong>floating point</strong> representation on 32 bits denoted by <a href=https://en.wikipedia.org/wiki/Single-precision_floating-point_format target=_blank rel="noopener noreffer"><em>fp32</em></a> or <em>single precision</em>. This representations reserves 32-bits for every number and is composed of 3 parts:</p><ol><li><strong>Sign bit $s$</strong>: 1 bit that represents the sign of the number.</li><li><strong>Mantissa $m$</strong>: 23 bits that represent the value of the number.</li><li><strong>Exponent $e$</strong>: 8 bits that represent the exponent of the number.</li></ol><p>The value of the number is calculated roughly as: $(-1)^{sign} \times 2^{e -127} \times (1 + m)$. This is a very simplified explanation of how numbers are represented in computers. Discussion on how operations are done in this representation and the problems with it is out of the scope of this blog post. The important part is what is mentioned above: <em>This representations reserves 32-bits for every number</em>. <em>Reserves 32-bits for every number</em>. <em>32-bits for every number</em>. <em><strong>32-bits</strong></em>. In case you still didn&rsquo;t get it, this means that every number is represented by 32-bits. This is a lot of space for a number. The most important question is: <strong>Do we need that ?.</strong> To answer this question, we need to understand what <em>need</em> means.</p><p>To be continued&mldr;</p><blockquote><p>December 25 2022: <em>It&rsquo;s christmas so i&rsquo;ll stop here and continue after the holidays</em>.</p></blockquote><h3 id=how-to-do-it>How to do it:</h3><h2 id=references>References</h2><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p><a href=https://arxiv.org/abs/2103.13630 target=_blank rel="noopener noreffer">https://arxiv.org/abs/2103.13630</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2022-12-25</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://wassimseifeddine.com/posts/nn_acceleration/ data-title="Neural network acceleration" data-hashtags=Quantization,Pruning><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://wassimseifeddine.com/posts/nn_acceleration/ data-hashtag=Quantization><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://wassimseifeddine.com/posts/nn_acceleration/ data-title="Neural network acceleration"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://wassimseifeddine.com/posts/nn_acceleration/ data-title="Neural network acceleration"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://wassimseifeddine.com/posts/nn_acceleration/ data-title="Neural network acceleration"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/quantization/>Quantization</a>,&nbsp;<a href=/tags/pruning/>Pruning</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/torchserve/ class=prev rel=prev title="[ARCHIVE] Serving BERT Model with Pytorch using TorchServe"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>[ARCHIVE] Serving BERT Model with Pytorch using TorchServe</a>
<a href=/posts/intro/ class=next rel=next title=Intro>Intro<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lunr@2.3.9/lunr.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1},search:{highlightTag:"em",lunrIndexURL:"/index.json",maxResultLength:10,noResultsFound:"No results found",snippetLength:30,type:"lunr"}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JKG8HGHF8S",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-JKG8HGHF8S" async></script></body></html>