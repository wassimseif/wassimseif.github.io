<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Torchtitan: A PyTorch Library for Parallelism Techniques explained - Wassim Seifeddine</title><meta name=Description content="Talking about Machine learning, mostly computational biology, bio foundation models and large scale training"><meta property="og:url" content="https://wassimseifeddine.com/posts/torchtitan/">
<meta property="og:site_name" content="Wassim Seifeddine"><meta property="og:title" content="Torchtitan: A PyTorch Library for Parallelism Techniques explained"><meta property="og:description" content="Torchtitan is an excellent project to learn how to implement distributed training techniques for training massive language models on hundreds of GPUs. I’ve been using it for a few months, and now that the paper is out, I thought it’d be a good idea to share a few posts about how to use it, what works and what doesn’t, what I learned while implementing these ideas in my own project."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-12-13T14:05:13+02:00"><meta property="article:modified_time" content="2024-12-13T14:05:13+02:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Torchtitan: A PyTorch Library for Parallelism Techniques explained"><meta name=twitter:description content="Torchtitan is an excellent project to learn how to implement distributed training techniques for training massive language models on hundreds of GPUs. I’ve been using it for a few months, and now that the paper is out, I thought it’d be a good idea to share a few posts about how to use it, what works and what doesn’t, what I learned while implementing these ideas in my own project."><meta name=twitter:site content="@https://twitter.com/WassSeif"><meta name=application-name content="Wassim Seifeddine"><meta name=apple-mobile-web-app-title content="Wassim Seifeddine"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://wassimseifeddine.com/posts/torchtitan/><link rel=prev href=https://wassimseifeddine.com/posts/blast_p1/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Torchtitan: A PyTorch Library for Parallelism Techniques explained","inLanguage":"en-us","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/wassimseifeddine.com\/posts\/torchtitan\/"},"genre":"posts","wordcount":833,"url":"https:\/\/wassimseifeddine.com\/posts\/torchtitan\/","datePublished":"2024-12-13T14:05:13+02:00","dateModified":"2024-12-13T14:05:13+02:00","publisher":{"@type":"Organization","name":""},"author":{"@type":"Person","name":"Author"},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"light"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"light"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="Wassim Seifeddine">Wassim Seifeddine</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/about/><i class="fa-solid fa-address-card"></i> About </a><a class=menu-item href=/portfolio/><i class="fa-solid fa-pepper-hot"></i> Portfolio </a><a class=menu-item href=/posts/><i class="fa-solid fa-candy-cane"></i> Posts </a><a class=menu-item href=/conferences/><i class="fa-solid fa-handshake"></i> Conferences </a><a class=menu-item href="https://scholar.google.com/citations?user=x6EK09kAAAAJ&amp;hl=en" rel="noopener noreffer" target=_blank><i class="fa-solid fa-magnifying-glass"></i> Publications </a><a class=menu-item href=/wassim_seifeddine_cv.pdf><i class="fa-solid fa-candy-cane"></i> Resume </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="Wassim Seifeddine">Wassim Seifeddine</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/about/ title><i class="fa-solid fa-address-card"></i>About</a><a class=menu-item href=/portfolio/ title><i class="fa-solid fa-pepper-hot"></i>Portfolio</a><a class=menu-item href=/posts/ title><i class="fa-solid fa-candy-cane"></i>Posts</a><a class=menu-item href=/conferences/ title><i class="fa-solid fa-handshake"></i>Conferences</a><a class=menu-item href="https://scholar.google.com/citations?user=x6EK09kAAAAJ&amp;hl=en" title rel="noopener noreffer" target=_blank><i class="fa-solid fa-magnifying-glass"></i>Publications</a><a class=menu-item href=/wassim_seifeddine_cv.pdf title><i class="fa-solid fa-candy-cane"></i>Resume</a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Torchtitan: A PyTorch Library for Parallelism Techniques explained</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw" aria-hidden=true></i>Author</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2024-12-13>2024-12-13</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;833 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;4 minutes&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-introduction>1. Introduction</a></li><li><a href=#2-why-is-this-important>2. Why is This Important?</a></li><li><a href=#3-scaling-techniques-in-torchtitan>3. Scaling Techniques in Torchtitan</a><ul><li><a href=#meta-initialization>Meta Initialization</a></li><li><a href=#float8-training>Float8 Training</a></li><li><a href=#pipeline-parallelism>Pipeline Parallelism</a></li><li><a href=#tensor-parallelism>Tensor Parallelism</a></li><li><a href=#fsdp2>FSDP2</a></li><li><a href=#regional-compilation>Regional Compilation</a></li><li><a href=#flight-recorder>Flight Recorder</a></li><li><a href=#fault-tolerance-training>Fault Tolerance Training</a></li><li><a href=#wandb-integration>Wandb Integration</a></li><li><a href=#other-features>Other Features</a></li></ul></li><li><a href=#3-next-steps>3. Next Steps</a></li></ul></nav></div></div><div class=content id=content><p>Torchtitan is an excellent project to learn how to implement distributed training techniques for training massive language models on <strong>hundreds</strong> of GPUs. I’ve been using it for a few months, and now that the <a href=https://arxiv.org/abs/2410.06511 target=_blank rel="noopener noreffer">paper</a> is out, I thought it’d be a good idea to share a few posts about how to use it, what works and what doesn’t, what I learned while implementing these ideas in my own project.</p><hr><p>This post will be an overview of the distributed training techniques implemented in Torchtitan and why they are important.
In the next couple of posts, I will be writing about my experience implementing each technique in my own project and what I learned from it and how much it actually helped.</p><h2 id=1-introduction>1. Introduction</h2><p>Torchtitan is a project from pytorch&rsquo;s team, it&rsquo;s their attempt to consolidate all the parallelism and distributed training techniques available in Pytorch into a single <em>framework</em> that can be used to train large models on hundreds of GPUs and they did an <strong>excellent</strong> job at it.</p><p>The techniques implemented in Torchtitan are modular, easy to understand and use, and they are all built on top of mostly vanilla Pytorch.</p><p>I use it more as a learning tool, to understand how to implement parallelism techniques in Pytorch, and to see how the Pytorch team is thinking about scaling deep learning models.</p><hr><h2 id=2-why-is-this-important>2. Why is This Important?</h2><p>Torchtitan simplifies a few things that were missing or hard to do <em>correctly</em> in PyTorch:</p><ol><li>A unified, central resource for all parallelism techniques (implemented by the PyTorch team).</li><li>Faster experimentation thanks to a clean, modular design.</li><li>Consolidation or improvement of a few libraries:<ul><li><strong>Pippy</strong> -> <code>torch.distributed.pipelining</code></li><li><strong>FSDP1</strong> -> <strong>FSDP2</strong></li><li><strong>FlatParameter</strong> -> <code>DTensor</code></li></ul></li></ol><hr><h2 id=3-scaling-techniques-in-torchtitan>3. Scaling Techniques in Torchtitan</h2><p>Torchtitan’s techniques are very modular. Think of them like building blocks you can add or remove from your sharding strategy. The full pipeline looks like this:</p><figure><img src=torch_titan_pipeline.png alt="Pipeline in Torchtitan" width=500 height=300><figcaption>Image from the <a href=https://arxiv.org/pdf/2410.06511>Torchtitan paper</a></figcaption></figure><p>This design makes it easier to experiment without too much code refactoring.</p><h3 id=meta-initialization>Meta Initialization</h3><p>This is the simplest technique. It initializes the model on an <em>empty</em> device, so the weights aren’t materialized until you apply the full sharding strategy. Saves a lot of time and memory headaches.</p><h3 id=float8-training>Float8 Training</h3><p>I haven’t tested this yet because I don’t have access to H100s. They use the <a href=https://github.com/pytorch/ao/tree/main/torchao/float8 target=_blank rel="noopener noreffer">torchao implementation</a>. The claim is that this will give you a huge speedup in training time (on the correct hardware) up to 50% in some cases. I will learn about this in the future and write a post about it when i test it.</p><figure><img src=float8_results.png alt="Float8 results" width=500 height=300><figcaption>Table 1 from the <a href=https://arxiv.org/pdf/2410.06511>paper</a></figcaption></figure><br><h3 id=pipeline-parallelism>Pipeline Parallelism</h3><p>This is the first step the &ldquo;sharding&rdquo; actualy happens. They divide the model into computation stages, each stage is sent to a device along with the required weights. The stages can also further split vertically ( more pipeline parallelism) or horizontally (tensor parallelism).</p><br><h3 id=tensor-parallelism>Tensor Parallelism</h3><p>They leverage the new Distributed Tensor (DTensor) to split a layer’s weight. This was once tricky to do in plain PyTorch.
This partitioniing allows sharded computation through PyTorch’s RowwiseParallel and ColwiseParallel APIs without changing the model code a lot.</p><p>They also use Asynchronous Tensor Parallel to further improve the GPU utilization by minimizing the time a GPU waits for new tensors coming from the communication link (NVLink, ethernet&mldr;).</p><br><h3 id=fsdp2>FSDP2</h3><p>They have improved on FSDP1 by improving the <a href=https://pytorch.org/docs/stable/fsdp.html target=_blank rel="noopener noreffer">FlatParamter</a> and replacing
it by a new <a href=https://pytorch.org/docs/stable/distributed.tensor.html target=_blank rel="noopener noreffer">implementation</a> called DTensor (Distributed Tensor). This new data type also used in Tensor Parallelism.
They have around 7% improvement over FSDP1 which is a nice free improvement to have.</p><p>They use FSDP in 2 cases :</p><ul><li>1D parallelism and CPU offloading.</li><li>Shard on node level if used with other parallelism techniques.</li></ul><h3 id=regional-compilation>Regional Compilation</h3><p>Instead of rely on Pytorch&rsquo;s Compiler <a href=https://pytorch.org/docs/stable/torch.compiler.html target=_blank rel="noopener noreffer">docs</a> to compile the
whole model and hope that it gets it optimized. They compile only specific blocks (TransformerBlock) where the attention is which gives them a simpler graph
and since they&rsquo;re compiling the same structe, they only have to compile it once and save a lot of compilation time.</p><h3 id=flight-recorder>Flight Recorder</h3><p>Comes very handy when you are debugging what nccl is doing.</p><h3 id=fault-tolerance-training>Fault Tolerance Training</h3><p>This is a very interesting feature, it allows your training loop to continue even if. I still have to test it.</p><h3 id=wandb-integration>Wandb Integration</h3><p>Although tensorboard is the default, they <a href=https://github.com/pytorch/torchtitan/pull/699 target=_blank rel="noopener noreffer">finally</a> have some wandb integration</p><h3 id=other-features>Other Features</h3><ul><li>Asynchronous Tensor Parallel</li><li>Sequence Parallel (?)</li><li>Context Parallel (?)</li></ul><p>Most of these features are controlled by the <code>ParallelDims</code> class implemented <a href=https://github.com/pytorch/torchtitan/blob/e846b6946783b2673b3ffc158474570265af9e90/torchtitan/parallelisms/parallel_dims.py#L15 target=_blank rel="noopener noreffer">here</a>. This lets you represent your model’s sharding strategy intuitively.</p><hr><h2 id=3-next-steps>3. Next Steps</h2><p>If you are interested in learning more about Torchtitan, I would recommend reading the <a href=https://arxiv.org/abs/2410.06511 target=_blank rel="noopener noreffer">paper</a> it&rsquo;s quite detailed and it explains the techniques implemented in the library in detail.</p><p>Also, checkout the Github <a href=https://github.com/pytorch/torchtitan target=_blank rel="noopener noreffer">repo</a>, In the supplementary materials of the paper you&rsquo;ll find code sections for a technique implemented so you can use this to understand the repository better.</p><div style="border-left:5px solid #fef9e7;background-color:#fef9e7;padding:15px 20px;margin:20px 0;font-size:1em;font-weight:light;color:#333;border-radius:8px;box-shadow:0 0 rgba(0,0,0,.1)">If you're interested in this topic or have any questions, feel free to reach out to me.</div>|
## 4.</div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>Updated on 2024-12-13</span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span><a href=javascript:void(0); title="Share on Twitter" data-sharer=twitter data-url=https://wassimseifeddine.com/posts/torchtitan/ data-title="Torchtitan: A PyTorch Library for Parallelism Techniques explained" data-via=https://twitter.com/WassSeif><i class="fab fa-twitter fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Facebook" data-sharer=facebook data-url=https://wassimseifeddine.com/posts/torchtitan/><i class="fab fa-facebook-square fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Hacker News" data-sharer=hackernews data-url=https://wassimseifeddine.com/posts/torchtitan/ data-title="Torchtitan: A PyTorch Library for Parallelism Techniques explained"><i class="fab fa-hacker-news fa-fw" aria-hidden=true></i></a><a href=javascript:void(0); title="Share on Line" data-sharer=line data-url=https://wassimseifeddine.com/posts/torchtitan/ data-title="Torchtitan: A PyTorch Library for Parallelism Techniques explained"><i data-svg-src=https://cdn.jsdelivr.net/npm/simple-icons@7.3.0/icons/line.svg aria-hidden=true></i></a><a href=javascript:void(0); title="Share on 微博" data-sharer=weibo data-url=https://wassimseifeddine.com/posts/torchtitan/ data-title="Torchtitan: A PyTorch Library for Parallelism Techniques explained"><i class="fab fa-weibo fa-fw" aria-hidden=true></i></a></span></div></div></div><div class=post-info-more><section class=post-tags></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/posts/blast_p1/ class=prev rel=prev title="How to install BLAST on mac and download the database"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>How to install BLAST on mac and download the database</a></div></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/sharer.js@0.5.1/sharer.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/copy-tex.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:50},comment:{},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!0,left:"\\begin{equation}",right:"\\end{equation}"},{display:!0,left:"\\begin{equation*}",right:"\\end{equation*}"},{display:!0,left:"\\begin{align}",right:"\\end{align}"},{display:!0,left:"\\begin{align*}",right:"\\end{align*}"},{display:!0,left:"\\begin{alignat}",right:"\\end{alignat}"},{display:!0,left:"\\begin{alignat*}",right:"\\end{alignat*}"},{display:!0,left:"\\begin{gather}",right:"\\end{gather}"},{display:!0,left:"\\begin{CD}",right:"\\end{CD}"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script type=text/javascript src=/js/theme.min.js></script><script type=text/javascript>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-JKG8HGHF8S",{anonymize_ip:!0})</script><script type=text/javascript src="https://www.googletagmanager.com/gtag/js?id=G-JKG8HGHF8S" async></script></body></html>