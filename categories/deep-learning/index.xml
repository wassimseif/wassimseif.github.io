<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Wassim Seifeddine</title><link>https://wassimseifeddine.com/categories/deep-learning/</link><description>Recent content in Deep Learning on Wassim Seifeddine</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Fri, 13 Dec 2024 14:05:13 +0200</lastBuildDate><atom:link href="https://wassimseifeddine.com/categories/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Torchtitan: A PyTorch Library for Parallelism Techniques explained</title><link>https://wassimseifeddine.com/posts/torchtitan/</link><pubDate>Fri, 13 Dec 2024 14:05:13 +0200</pubDate><guid>https://wassimseifeddine.com/posts/torchtitan/</guid><description>&lt;p&gt;Torchtitan is an excellent project to learn how to implement distributed training techniques for training massive language models on &lt;strong&gt;hundreds&lt;/strong&gt; of GPUs. I’ve been using it for a few months, and now that the &lt;a href="https://arxiv.org/abs/2410.06511" target="_blank" rel="noopener noreffer "&gt;paper&lt;/a&gt; is out, I thought it’d be a good idea to share a few posts about how to use it, what works and what doesn’t, what I learned while implementing these ideas in my own project.&lt;/p&gt;</description></item><item><title>Neural network acceleration</title><link>https://wassimseifeddine.com/posts/nn_acceleration/</link><pubDate>Sun, 25 Dec 2022 17:55:13 +0200</pubDate><guid>https://wassimseifeddine.com/posts/nn_acceleration/</guid><description>&lt;h2 id="why-accelerate"&gt;Why accelerate&lt;/h2&gt;
&lt;p&gt;Well, we have neural networks, they are awesome, they work but there&amp;rsquo;s a problem. THEY ARE &lt;strong&gt;HUGE&lt;/strong&gt;. We scaled from a hundred of millions of parameters to hundred of &lt;strong&gt;BILLIONS&lt;/strong&gt;. This problem makes using neural networks in real life quite hard as you normally don&amp;rsquo;t have this huge computational capabilities to run them anywhere.&lt;/p&gt;
&lt;p&gt;Neural networks have proven to be a very valuable tool in scenarios where the transformation from inputs to outputs is unknown. Suppose you are asked to write an algorithm to classify an image if it&amp;rsquo;s a cat or a dog, how would you do that ? Well first you might ask yourself, &amp;ldquo;what makes an image a cat?&amp;rdquo;. Answering this question is incredibly hard because a vast amount of cases to cover in order to have your algorithm generalizable. This is where neural networks shine; Given an input $ x_{i} $ with its respective label $ y_{i}$ you can use a neural network model with a set of parameters $\theta$ denoted by $ M(\theta) $ to approximate $y_{i} = f(x_{i})$. Normally with enough data you can get a very good estimate of $f$. &lt;strong&gt;However&lt;/strong&gt;, this comes at a huge cost, training and running these large networks is expensive in terms of time and memory because of the huge amount of parameters that you need to learn to get the best approximation, this makes these models hard to use in real life scenarios. Also, the recent trend of models getting bigger and bigger in order to get better performance is making this problem even harder.&lt;/p&gt;</description></item><item><title>[ARCHIVE] Using Tensorflow Gradient Descent to minimize functions</title><link>https://wassimseifeddine.com/posts/sgd/</link><pubDate>Sun, 02 Oct 2016 17:54:04 +0100</pubDate><guid>https://wassimseifeddine.com/posts/sgd/</guid><description>&lt;p&gt;You probably heard the hype around Gradient Descent &amp;amp; how it&amp;rsquo;s used to optimize &lt;em&gt;Deep Learning Models&lt;/em&gt;. But you may never knew that it can be used to minimize &lt;strong&gt;ANY&lt;/strong&gt; function you know.&lt;/p&gt;
&lt;p&gt;In this blog post we&amp;rsquo;re going to see how to use Gradient Descent to optimize a simple quadratic function &lt;!-- more --&gt;&lt;/p&gt;
&lt;p&gt;we all know from highschool.&lt;/p&gt;
&lt;div style="text-align: center;"&gt;
&lt;p&gt;$ax^2 + bx + c$&lt;/p&gt;
&lt;/div&gt;
with
&lt;div class="highlight"&gt;&lt;pre tabindex="0" class="chroma"&gt;&lt;code class="language-python" data-lang="python"&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;b&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;40&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class="line"&gt;&lt;span class="cl"&gt;&lt;span class="n"&gt;c&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;400&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we get the following formula
&lt;br&gt;&lt;/p&gt;</description></item></channel></rss>